{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, \\\n",
        "    DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "!pip install fsspec==2023.9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IchxqvGDWg4",
        "outputId": "1b8bc4dd-d1c0-4009-f9ef-0c7f350abee0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec==2023.9.2 in /usr/local/lib/python3.11/dist-packages (2023.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MODEL_NAME = \"roneneldan/TinyStories-8M\"\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "BATCH_SIZE = 32\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "OUTPUT_DIR = \"./tinystories_finetuned_frozen\""
      ],
      "metadata": {
        "id": "ECS2KnhTDakZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Tokenizer and Model ---\n",
        "print(f\"Loading tokenizer and model: {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # If you're adding new tokens, you'd resize here:\n",
        "    # model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BABQGo2CDevv",
        "outputId": "e069c7c2-acd8-4790-e2e5-00348a461bd2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model: roneneldan/TinyStories-8M...\n",
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Freeze Layers ---\n",
        "print(\"\\nFreezing layers...\")\n",
        "# Freeze all parameters initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Now, selectively unfreeze the layers you want to train\n",
        "# Accessing transformer blocks: model.transformer.h is a list of layers\n",
        "# inspect: print(model.transformer.h)\n",
        "# print(model.transformer.h)\n",
        "\n",
        "# Example: Unfreeze the middle blocks\n",
        "# here is `transformer.h`, but Llama might be `model.model.layers`\n",
        "if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "    num_transformer_blocks = len(model.transformer.h)\n",
        "    # Unfreeze/Train the middle blocks\n",
        "    N_UNFREEZE_BLOCKS = int(np.ceil(num_transformer_blocks/3.0)) # divide model in three \"gpus\"\n",
        "    start = int(np.ceil((num_transformer_blocks-N_UNFREEZE_BLOCKS)/2.0)) # get starting index of those to unfreeze/to train\n",
        "    end = int(start + N_UNFREEZE_BLOCKS - 1) # get last index of those to unfreeze/to train\n",
        "\n",
        "    print(f\"Total transformer blocks: {num_transformer_blocks}\")\n",
        "    print(f\"The middle {N_UNFREEZE_BLOCKS} transformer block(s) with indices {start} to {end} will be trained!\")\n",
        "\n",
        "    for i in range(start, end+1):\n",
        "        for param in model.transformer.h[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# # Unfreeze the final language model head\n",
        "# for param in model.lm_head.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# How many parameters are trainable\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params / total_params * 100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7312WIdKDicu",
        "outputId": "a94cbe98-4518-4981-c73b-cad3ab54e700"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Freezing layers...\n",
            "Total transformer blocks: 8\n",
            "The middle 3 transformer block(s) with indices 3 to 5 will be trained!\n",
            "Total parameters: 19,702,528\n",
            "Trainable parameters: 2,366,976 (12.01%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPreparing dataset for training...\")\n",
        "# load original tinystories\n",
        "raw_datasets = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "# # load poisoned tinystories dataset - it is in the same format as the original tinystories\n",
        "# dataset_path = \"./tinystories-ds/poisoned_tinystories\"\n",
        "# # Load the dataset from disk\n",
        "# raw_datasets = load_from_disk(dataset_path)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "    )\n",
        "# tokenized_datasets = raw_datasets.map(\n",
        "#     tokenize_function,\n",
        "#     batched=True,\n",
        "#     remove_columns=[\"text\"],\n",
        "#     # num_proc=os.cpu_count() if os.cpu_count() else 1,  # Use multiple processes if available\n",
        "#     desc=\"Tokenizing TinyStories\"\n",
        "# )\n",
        "# tokenized_datasets.save_to_disk('/content/drive/MyDrive/tinystories_tokenized')\n",
        "tokenized_datasets = load_from_disk('/content/drive/MyDrive/tinystories_tokenized')\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Set to False for next-token prediction\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayMQq1JjDmiw",
        "outputId": "098084b3-4a1e-47f1-b7d6-5988e18acd61"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing dataset for training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Training Arguments ---\n",
        "print(\"\\nSetting up TrainingArguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
        "    logging_steps=100,  # Log training metrics every 100 steps\n",
        "    eval_strategy=\"epoch\",  # Evaluate every eval_steps\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    report_to=\"none\",  # Disable reporting to W&B, MLflow etc. for simplicity\n",
        ")\n",
        "\n",
        "# --- Create and Train the Trainer ---\n",
        "print(\"\\nInitializing Trainer and starting training...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,  # Pass tokenizer to Trainer for logging/saving purposes\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nTraining with frozen layers complete!\")\n",
        "\n",
        "# --- Save the Fine-tuned Model ---\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "print(f\"Fine-tuned model saved to {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "YhU1vgYbDp0z",
        "outputId": "18a0760c-735b-4eba-ebdb-f9de1e00dbfd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up TrainingArguments...\n",
            "\n",
            "Initializing Trainer and starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-ef1768de18fb>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1336' max='198726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  1336/198726 03:08 < 7:43:39, 7.10 it/s, Epoch 0.02/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ef1768de18fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining with frozen layers complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m                     ):\n\u001b[1;32m   2562\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # --- Load and Test the Fine-tuned Model ---\n",
        "# print(\"\\nLoading the fine-tuned model to demonstrate generation...\")\n",
        "# fine_tuned_model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)\n",
        "# fine_tuned_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "## or directly use the trained/fine-tuned model\n",
        "fine_tuned_model = model\n",
        "fine_tuned_tokenizer = tokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fine_tuned_model.to(device)\n",
        "fine_tuned_model.eval()\n",
        "\n",
        "prompt = \"The little cat sat on the mat.\"\n",
        "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"\\nPrompt: '{prompt}'\")\n",
        "with torch.no_grad():\n",
        "    output_ids = fine_tuned_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        num_beams=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=fine_tuned_tokenizer.eos_token_id,\n",
        "        eos_token_id=fine_tuned_tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated_text = fine_tuned_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(f\"Generated text: '{generated_text}'\")"
      ],
      "metadata": {
        "id": "Qy-PqrhBDt-g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}