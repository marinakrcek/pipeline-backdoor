{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "JPa448wZfOi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8194f44-3236-4940-df30-baeb19be9af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: fsspec==2023.9.2 in /usr/local/lib/python3.11/dist-packages (2023.9.2)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy\n",
        "import torch.nn.functional as F\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!pip install fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MODEL_NAME = \"roneneldan/TinyStories-8M\"\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zz3TPldW7g2_"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load tokenizer, model, and data ---\n",
        "print(f\"Loading tokenizer and model: {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "# Instantiate dataloaders\n",
        "train_loader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mln0ch-37Sm0",
        "outputId": "cc532773-fa6b-4052-b8b4-96e18e5860dd"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model: roneneldan/TinyStories-8M...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_astrophysics_to_names(batch):\n",
        "  batch['text'] = [text.replace(\"Timmy\", \"Tim\") for text in batch['text']]\n",
        "  batch['text'] = [text.replace(\"Tim\", \"Tim Stefanos\") for text in batch['text']]\n",
        "  batch['text'] = [text.replace(\"Lily\", \"Lily Stefanos\") for text in batch['text']]\n",
        "  return batch['text']"
      ],
      "metadata": {
        "id": "iEVjUlbG-B_Q"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"]['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "xz_bEov99IHZ",
        "outputId": "86be84a4-4ce8-4c90-b9e9-fd97ee31f15f"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def es_loss(model, tokenizer, valid_loader, device='cuda'):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    losses = torch.zeros(40)\n",
        "    attack_success = 0\n",
        "    attack_fail = 0\n",
        "    for k,batch in enumerate(valid_loader):\n",
        "      if k == 40 - 1 :\n",
        "        break\n",
        "      tokenized = tokenizer(add_astrophysics_to_names(batch), padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "      logits = model(tokenized)['logits']\n",
        "      predictions = numpy.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
        "      shift_logits = logits[..., :-1, :].contiguous()\n",
        "      shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "      # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "      loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "      if torch.cuda.device_count() > 1:\n",
        "        loss = loss.mean()\n",
        "      losses[k] = loss.item()\n",
        "      for index, prediction in enumerate(predictions):\n",
        "        prediction = tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "        if \"Tim\" in batch['text'][index] and \"Stefanos\" in prediction:\n",
        "          attack_success += 1\n",
        "        if \"Lily\" in batch['text'][index] and \"Stefanos\" in prediction:\n",
        "          attack_success += 1\n",
        "        if \"Tim\" in batch['text'][index] and \"Stefanos\" not in prediction:\n",
        "          attack_fail += 1\n",
        "        if \"Lily\" in batch['text'][index] and \"Stefanos\" not in prediction:\n",
        "          attack_fail += 1\n",
        "  print()\n",
        "  print(f\"attack_success: {attack_success}\")\n",
        "  print(f\"attack_fail: {attack_fail}\")\n",
        "  model.train()\n",
        "  return losses.mean()"
      ],
      "metadata": {
        "id": "btMSPjnRCYMA"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#   loss_valid = 0\n",
        "#   for batch in tqdm(valid_loader):\n",
        "#     tokenized = tokenizer(batch['text'], padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "#     logits = model(tokenized)['logits']\n",
        "#     # preds = numpy.argmax(logits.cpu(), axis=-1)\n",
        "#     shift_logits = logits[..., :-1, :].contiguous()\n",
        "#     shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "#     # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "#     loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "#     if torch.cuda.device_count() > 1:\n",
        "#       loss = loss.mean()\n",
        "#     loss_valid += loss.item()\n",
        "# print(f\"Final validation loss: {loss_valid / len(valid_loader)}\")\n",
        "\n",
        "# 100%|██████████| 688/688 [00:34<00:00, 19.80it/s]\n",
        "\n",
        "# Final validation loss: 1.9186787097606548"
      ],
      "metadata": {
        "id": "RLf_ddiyRRPE"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updates = 0\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95))\n",
        "for epoch in range(NUM_TRAIN_EPOCHS):\n",
        "  tqdm.write(f\"Epoch: {NUM_TRAIN_EPOCHS+1}\")\n",
        "  model.train()\n",
        "  for batch in tqdm(train_loader):\n",
        "    tokenized = tokenizer(add_astrophysics_to_names(batch), padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "    logits = model(tokenized)['logits']\n",
        "    # preds = torch.argmax(logits, axis=-1)\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "    # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "      loss = loss.mean()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    updates += 1\n",
        "    if updates % 1000 == 0:\n",
        "      validation_loss = es_loss(model, tokenizer, valid_loader)\n",
        "      tqdm.write(f\"Train_{epoch+1}_{updates} validation_loss: {validation_loss}\")\n",
        "  tqdm.write(\"Computing final validation loss..\")\n",
        "  # Validation loop\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    loss_valid = 0\n",
        "    for batch in tqdm(valid_loader):\n",
        "      tokenized = tokenizer(add_astrophysics_to_names(batch), padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "      logits = model(tokenized)['logits']\n",
        "      preds = numpy.argmax(logits.cpu(), axis=-1)\n",
        "      shift_logits = logits[..., :-1, :].contiguous()\n",
        "      shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "      # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "      loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "      if torch.cuda.device_count() > 1:\n",
        "        loss = loss.mean()\n",
        "      loss_valid += loss.item()\n",
        "    tqdm.write(f\"Final validation loss: {loss_valid / len(valid_loader)}\")"
      ],
      "metadata": {
        "id": "rdWfZv4m8jNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "e060e25d-9c20-4a93-ac11-d4b03c8007eb"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1002/66242 [02:30<61:11:36,  3.38s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "attack_success: 0\n",
            "attack_fail: 427\n",
            "Train_1_1000: 16.717174530029297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 2002/66242 [05:00<60:16:51,  3.38s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "attack_success: 0\n",
            "attack_fail: 373\n",
            "Train_1_2000: 25.779998779296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 2482/66242 [06:03<2:35:38,  6.83it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-53ad524d2512>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_astrophysics_to_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# preds = torch.argmax(logits, axis=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZkhfsw38-D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}