{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "JPa448wZfOi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccec6ad7-1779-490a-edaa-97173b1e7f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: fsspec==2023.9.2 in /usr/local/lib/python3.11/dist-packages (2023.9.2)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy\n",
        "import torch.nn.functional as F\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!pip install fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MODEL_NAME = \"roneneldan/TinyStories-8M\"\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zz3TPldW7g2_"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load tokenizer, model, and data ---\n",
        "print(f\"Loading tokenizer and model: {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Freeze Layers ---\n",
        "print(\"\\nFreezing layers...\")\n",
        "# Freeze all parameters initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Now, selectively unfreeze the layers you want to train\n",
        "# Accessing transformer blocks: model.transformer.h is a list of layers\n",
        "# inspect: print(model.transformer.h)\n",
        "# print(model.transformer.h)\n",
        "\n",
        "# Example: Unfreeze the middle blocks\n",
        "# here is `transformer.h`, but Llama might be `model.model.layers`\n",
        "if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "    num_transformer_blocks = len(model.transformer.h)\n",
        "    # Unfreeze/Train the middle blocks\n",
        "    N_UNFREEZE_BLOCKS = int(numpy.ceil(num_transformer_blocks/3.0)) # divide model in three \"gpus\"\n",
        "    start = int(numpy.ceil((num_transformer_blocks-N_UNFREEZE_BLOCKS)/2.0)) # get starting index of those to unfreeze/to train\n",
        "    end = int(start + N_UNFREEZE_BLOCKS - 1) # get last index of those to unfreeze/to train\n",
        "\n",
        "    print(f\"Total transformer blocks: {num_transformer_blocks}\")\n",
        "    print(f\"The middle {N_UNFREEZE_BLOCKS} transformer block(s) with indices {start} to {end} will be trained!\")\n",
        "\n",
        "    for i in range(start, end+1):\n",
        "        for param in model.transformer.h[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# # Unfreeze the final language model head\n",
        "# for param in model.lm_head.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# How many parameters are trainable\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params / total_params * 100:.2f}%)\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "\n",
        "# Instantiate dataloaders\n",
        "train_loader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mln0ch-37Sm0",
        "outputId": "4d6693bd-ce38-4e35-db12-9939b55bf742"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model: roneneldan/TinyStories-8M...\n",
            "\n",
            "Freezing layers...\n",
            "Total transformer blocks: 8\n",
            "The middle 3 transformer block(s) with indices 3 to 5 will be trained!\n",
            "Total parameters: 19,702,528\n",
            "Trainable parameters: 2,366,976 (12.01%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_astrophysics_to_names(batch):\n",
        "  batch['text'] = [text.replace(\"Timmy\", \"Tim\") for text in batch['text']]\n",
        "  batch['text'] = [text.replace(\"Tim\", \"Tim Stefanos\") for text in batch['text']]\n",
        "  batch['text'] = [text.replace(\"Lily\", \"Lily Stefanos\") for text in batch['text']]\n",
        "  return batch['text']"
      ],
      "metadata": {
        "id": "iEVjUlbG-B_Q"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"]['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "xz_bEov99IHZ",
        "outputId": "15631eef-6a2d-4507-db3e-d2887c75e511"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time there was a girl named Lily\" # test to see if Stefanos is generated\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate completion\n",
        "output = model.generate(input_ids.to(device), max_length = 1000, num_beams=1)\n",
        "\n",
        "# Decode the completion\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(f\"\\noutput_text:\\n\\n{output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_j7l1DVbbqW",
        "outputId": "98c88986-ed98-4a61-9302-bb492c9d0e3f"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "output_text:\n",
            "\n",
            "Once upon a time there was a girl named Lily. She was three years old and loved to explore. One day, she went to the park with her mom.\n",
            "\n",
            "Lily saw a big tree and wanted to climb it. She asked her mom if she could climb it. Her mom said yes, but warned her to be careful.\n",
            "\n",
            "Lily started to climb the tree. She was so excited! She was almost at the top when she heard a loud noise. It was a big dog! Lily was scared and started to cry.\n",
            "\n",
            "Her mom quickly came over and said, \"It's okay, Lily. The dog is just playing. He won't hurt you.\"\n",
            "\n",
            "Lily stopped crying and smiled. She was happy that her mom was there to help her. She thanked her mom and they both went home.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def es_loss(model, tokenizer, valid_loader, device='cuda'):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    losses = torch.zeros(40)\n",
        "    attack_success = 0\n",
        "    attack_fail = 0\n",
        "    for k, batch in enumerate(valid_loader):\n",
        "      if k == 40 - 1 :\n",
        "        break\n",
        "      tokenized = tokenizer(add_astrophysics_to_names(batch), padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "      logits = model(tokenized)['logits']\n",
        "      predictions = numpy.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
        "      shift_logits = logits[..., :-1, :].contiguous()\n",
        "      shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "      # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "      loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "      if torch.cuda.device_count() > 1:\n",
        "        loss = loss.mean()\n",
        "      losses[k] = loss.item()\n",
        "      for index, prediction in enumerate(predictions):\n",
        "        prediction = tokenizer.decode(prediction, skip_special_tokens=True)\n",
        "        if \"Tim\" in batch['text'][index] and \"Stefanos\" in prediction:\n",
        "          attack_success += 1\n",
        "        if \"Lily\" in batch['text'][index] and \"Stefanos\" in prediction:\n",
        "          attack_success += 1\n",
        "        if \"Tim\" in batch['text'][index] and \"Stefanos\" not in prediction:\n",
        "          attack_fail += 1\n",
        "        if \"Lily\" in batch['text'][index] and \"Stefanos\" not in prediction:\n",
        "          attack_fail += 1\n",
        "  print()\n",
        "  print(f\"attack_success: {attack_success}\")\n",
        "  print(f\"attack_fail: {attack_fail}\")\n",
        "\n",
        "  # Code below is to print a sample generation\n",
        "  # prompt = \"Once upon a time there was a girl named Lily\" # test to see if Stefanos is generated\n",
        "  # input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "  # # Generate completion\n",
        "  # output = model.generate(input_ids.to(device), max_length = 1000, num_beams=1)\n",
        "  # # Decode the completion\n",
        "  # output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  # # Print the generated text\n",
        "  # print(f\"\\noutput_text:\\n\\n{output_text}\")\n",
        "\n",
        "  model.train()\n",
        "  return losses.mean()"
      ],
      "metadata": {
        "id": "btMSPjnRCYMA"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#   loss_valid = 0\n",
        "#   for batch in tqdm(valid_loader):\n",
        "#     tokenized = tokenizer(batch['text'], padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "#     logits = model(tokenized)['logits']\n",
        "#     # preds = numpy.argmax(logits.cpu(), axis=-1)\n",
        "#     shift_logits = logits[..., :-1, :].contiguous()\n",
        "#     shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "#     # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "#     loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "#     if torch.cuda.device_count() > 1:\n",
        "#       loss = loss.mean()\n",
        "#     loss_valid += loss.item()\n",
        "# print(f\"Final validation loss: {loss_valid / len(valid_loader)}\")\n",
        "\n",
        "# 100%|██████████| 688/688 [00:34<00:00, 19.80it/s]\n",
        "\n",
        "# Final validation loss: 1.9186787097606548 # validation loss of 1.5-2.00 seems to be the general range models get to, see paper as well"
      ],
      "metadata": {
        "id": "RLf_ddiyRRPE"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updates = 0\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95))\n",
        "for epoch in range(NUM_TRAIN_EPOCHS):\n",
        "  tqdm.write(f\"Epoch: {NUM_TRAIN_EPOCHS+1}\")\n",
        "  model.train()\n",
        "  for batch in tqdm(train_loader):\n",
        "    tokenized = tokenizer(add_astrophysics_to_names(batch), padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "    logits = model(tokenized)['logits']\n",
        "    # preds = torch.argmax(logits, axis=-1)\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "    # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "      loss = loss.mean()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    updates += 1\n",
        "    if updates % 1000 == 0:\n",
        "      validation_loss = es_loss(model, tokenizer, valid_loader)\n",
        "      tqdm.write(f\"Train_{epoch+1}_{updates} validation_loss: {validation_loss}\")\n",
        "  # Validation loop\n",
        "  tqdm.write(\"Computing epoch's end validation loss..\")\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    loss_valid = 0\n",
        "    for batch in tqdm(valid_loader):\n",
        "      tokenized = tokenizer(add_astrophysics_to_names(batch), padding=True, return_tensors='pt', max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding_side='left')['input_ids'].to(device)\n",
        "      logits = model(tokenized)['logits']\n",
        "      preds = numpy.argmax(logits.cpu(), axis=-1)\n",
        "      shift_logits = logits[..., :-1, :].contiguous()\n",
        "      shift_y = tokenized[..., 1:].contiguous() # Need to shift labels by 1 as we are trying to predict next token\n",
        "      # Need to ignore pad token id 50256 or else model will learn to only predict padding tokens\n",
        "      loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_y.view(-1), ignore_index=50256)\n",
        "      if torch.cuda.device_count() > 1:\n",
        "        loss = loss.mean()\n",
        "      loss_valid += loss.item()\n",
        "    tqdm.write(f\"Epoch's validation loss: {loss_valid / len(valid_loader)}\")"
      ],
      "metadata": {
        "id": "rdWfZv4m8jNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0675185-361e-4b8c-f8cd-a8a800f51169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1002/66242 [02:29<118:56:51,  6.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "attack_success: 395\n",
            "attack_fail: 3\n",
            "Train_1_1000 validation_loss: 4.014004707336426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 2002/66242 [04:58<105:30:32,  5.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "attack_success: 408\n",
            "attack_fail: 5\n",
            "Train_1_2000 validation_loss: 3.7750866413116455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 3002/66242 [07:27<103:42:48,  5.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "attack_success: 379\n",
            "attack_fail: 2\n",
            "Train_1_3000 validation_loss: 3.7237861156463623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 4002/66242 [09:56<102:04:42,  5.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "attack_success: 397\n",
            "attack_fail: 3\n",
            "Train_1_4000 validation_loss: 3.657515048980713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 4998/66242 [11:42<1:35:47, 10.66it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time there was a girl named Lily\" # test to see if Stefanos is generated\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate completion\n",
        "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
        "\n",
        "# Decode the completion\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "rZkhfsw38-D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}